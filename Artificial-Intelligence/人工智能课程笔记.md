# 机器学习(Machine Learning)
---
> Improve on *T*ask with respect to *P*erformance metric based on *E*xperience

- 主要从效果(Effectiveness)和效率(efficiency of the computation)两方面衡量

机器学习的必要性：未知环境和复杂系统，大规模数据挖掘，促进人类智能

**Objectiveness**和**Optimization**

## 监督学习
> [!SUMMARY] 基本思想
> 给定$x$和$f(x)$，试图求解出一个假设的(*hypothesis*)映射函数$x$逼近$f$，其主要任务如下：
> 1. 如何定义函数形式
> 2. 如何优化函数参数和函数结构

- Mean Square Error(MSE)：均值平方根误差

Ockham's Razor(奥卡姆剃刀原理)：在逼近数据的前提下令函数尽可能简单，使函数具有普适性

### 决策树(Decision Tree)
选择作为根节点的特征：以某个特征能将子节点分成几个独立性更强的部分

- Entropy(熵)：从函数**结果**进行计算$$I(\frac p{p+n},\frac n{p+n})=-\frac p{p+n}\log_2\frac p{p+n}-\frac n{p+n}\log_2\frac n{p+n}$$熵越大表示区分数据越混乱，越小表示越独立
- Remainder Entropy(剩余熵)：$$R(A) = \sum_{i=1}^v\frac{p_i+n_i}{p+n}I(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i})$$
- 信息增益(Information Gain)：选择`IG`最大的特征作为根节点特征 $$IG(A) = I\left(\frac p{p+n},\frac n{p+n}\right)- R(A)$$

#### 预防过拟合(Overfitting)
1. 扩大数据集
2. 提前停止拟合
3. 允许在数据集上过拟合，但随后对模型做剪枝
   - 通过做DL(Description Length)分析，选取Code Length更低的决策树

### 贝叶斯推论
> [!NOTE] 贝叶斯定理
> $$P\left(h|D\right)=\frac{P\left(D,h\right)}{P\left(D\right)}=\frac{P\left(D|h\right)P\left(h\right)}{P\left(D\right)}$$其中$D$表示数据，$h$表示假设；$P(h|D)$表示后验概率(Posterior Probability)，$P(h)$表示先验概率(Prior Probability)，$P(D|h)$表示类条件概率(Class-conditional Probability)

- MAP(Maximum A Posterior)：最大后验估计，通过计算每个候选假设的后验概率找到最大项

- 简化类条件概率$P(D|h)$的形式方法：
  1. 朴素贝叶斯分类器(Naive Bayesian Classifiers, NBC)：假设各分量间没有关系，无需考虑协方差，每个分量都只有均值和方差需要考虑
  2. 贝叶斯信念网络(Bayesian Belief Classification, BBC)：将有关系的分量建立图模型联系
  3. 高斯混合模型(Gaussian Miature Model, GMM)：用简单的函数叠加形成复杂函数

#### 朴素贝叶斯分类器(NBC)
$$P\Big(D\Big|h\Big)=P\Big(d_1,\cdots,d_n\Big|h\Big)=\prod_iP\Big(d_i\Big|h\Big)$$即可以只看做各个特征概率之间的连乘

#### 贝叶斯信念网络(BBN)
在给定条件$Z$的时候假设特征$X$与$Y$相互独立，即$$P\Big(x_1,\cdots,x_l\Big|y_1,\cdots,y_m,z_1,\cdots,z_n\Big)=P\Big(x_1,\cdots,x_l\Big|z_1,\cdots,z_n\Big)$$

集合概率本质上是一个概率分布的表达形式，可以表示为$$P\left(x_1,\cdots,x_n\right)=\prod_{i=1}^nP\left(x_i\right|Parents\left(x_i\right)$$可以用有向图或条件概率表(Conditional Probability Table, CPT)表示![](Artificial-Intelligence/Pasted%20image%2020240602012513.png)
- 既能对概率分布做有效表达，又能限制其复杂性，使规模增长性由指数变为线性增长

- 编辑概率公式：$$P(x|y)=\alpha P(x,y) = \alpha\sum\limits_zP(x,y,z)$$其中$\alpha=P(y)^{-1}$，简单理解就是把$z$变量的所有取值都考虑一遍，这样相当于特征$z$不会影响到原有的情况。例如可以有$$\begin{align} P(B|j,m)&=\frac{P(B,j,m)}{P(j,m)}\\&=\alpha P(B,j,m)\\&=\alpha\sum\limits_{a}\sum\limits_{e}P(B,e,a,j,m) \end{align}$$

- 优化目标：极大似然估计(Maximum Likelihood Estimation, MLE)
- 优化方法：梯度上升$$\frac{\partial\ln P_w(D)}{\partial w_{ijk}}=\sum_{d\in D}\frac{P_w\left(x_{ij},u_{ik}|d\right)}{w_{ijk}}$$

## 无监督学习
### 聚类
1. 计算度量簇(cluster)和数据之间的相似性：欧氏距离(Euclidean)，余弦夹角，切比雪夫(Chebyishev，多维中每一维中最大的距离)，闵可夫斯基距离(Minkowski)
2. 划分数据集
3. 确保聚簇算法具有可伸缩性

### 分割聚类
#### K-means聚类
可以直接看[K-means(K均值)算法](Classification-Algorithm/分类算法学习.md#K-means(K均值)算法)
1. 随机选取$k$个数据作为初始聚簇中心
2. 把其他数据划分到距离最近的聚簇中
3. 对新的聚簇重新计算均值聚簇中心
4. 重复Step2和3

- 优化方程：$$J\Big(X,\nu\Big) = \sum_{i=1}^{n}\sum_{i=1}^{k}\Big(u_{ij}\Big)^{m} {\rho\Big(d\Big(x_{i},\nu_{j}\Big)\Big)}$$其中$v_{j}$表示聚簇中心，$\rho(d(x_{i}, v_{j})) = ||x_{i}-v_{j}||^{2}$，$u_{ij} \in \{0,1\}$
- 当将优化方程中的$m=1,2,3\cdots$，$u_{ij}\in[0,1]$，即设$u_{ij}$为数据点$x_{i}$数据聚簇中心$v_{j}$的模糊度，此时方法称为模糊k均值(Fuzzy k-means, FCM)

- k-means算法的特征
  1. 线性算法复杂度：$O(tkn)$
  2. 局部最优
  3. 需要指定聚类的个数
  4. 对噪声和初始聚类中心的确定比较敏感

#### K-medoids聚类
K-中心点聚类，也叫做PAM(Partitoning Around Medoids)

方法上寻找每个聚簇的代表性数据，特点上能够很好的**减少噪声对聚类划分的影响**，是在确定中心点和计算聚类中心之间的迭代

1. 随机选取$k$个中心点
2. 计算每组中心点和其他数据的替代成本设为$TC_{ih}$
3. 如果最小的$TC_{ih}<0$，就用$h$替换中心点$i$
4. 重复Step2和3，直到$TC_{ih}\geq 0$

# 人工神经网络(Artificial Neural Network)
---



# 搜索与问题求解(Search and Problem Solving)
---




# 知识与推理(Knowledge and Reasoning)
---



# 进化计算(Evolutionary Computation)
---



# 群体智能(Swarm Intelligence)
---




# 新型人工智能(Nouvelle AI)
---
