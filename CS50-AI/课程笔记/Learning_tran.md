# 第四讲

## 机器学习

机器学习为计算机提供数据，而不是明确的指令。使用这些数据，计算机学会识别模式并能够自行执行任务。

## 监督学习

监督学习是一项任务，其中计算机学习基于输入输出对的数据集将输入映射到输出的函数。

监督学习有多种任务，其中之一是**分类**。这是函数将输入映射到离散输出的任务。例如，给定特定一天的湿度和气压的一些信息（输入），计算机决定当天是否会下雨（输出）。计算机在对数据集进行多天训练后执行此操作，其中湿度和气压已经映射到是否下雨。

该任务可以形式化如下。我们观察大自然，其中函数*f（湿度、压力）*将输入映射到离散值，“Rain”（雨）或“No Rain”（无雨）。这个函数对我们来说是隐藏的，并且它可能受到许多我们无法访问的其他变量的影响。我们的目标是创建可以近似函数*F*行为的函数*h(湿度、压力)*。这样的任务可以通过在湿度和降雨维度（输入）上绘制天数来可视化，如果当天下雨，则将每个数据点着色为蓝色，如果当天没有下雨，则将每个数据点着色为红色（输出）。白色数据点只有输入，计算机需要计算出输出。

![分类](https://cs50.harvard.edu/ai/2024/notes/4/classification.png)

## 最近邻分类

解决上述任务的一种方法是为相关变量分配最接近观察值的值。因此，例如，上图中的白点将被涂成蓝色，因为最近观察到的点也是蓝色的。有时这可能会很有效，但请考虑下图。

![最近邻分类](https://cs50.harvard.edu/ai/2024/notes/4/nearestneighbor.png)

按照相同的策略，白点应该被涂成红色，因为距离它最近的观察点也是红色的。然而，从更大的角度来看，周围的大多数其他观察结果似乎都是蓝色的，这可能会让我们直觉认为在这种情况下蓝色是更好的预测，即使最接近的观察结果是红色的。

解决最近邻分类限制的一种方法是使用**k 最近邻分类**，其中根据*k*最近邻的最常见颜色对点进行着色。由程序员决定*k*是什么。例如，使用 3 个最近邻分类，上面的白点将被涂成蓝色，直观上这似乎是一个更好的决定。

k 最近邻分类的缺点是，使用简单的方法，算法必须测量每个点到相关点的距离，这在计算上是昂贵的。通过使用能够更快地找到邻居的数据结构或通过修剪不相关的观察可以加快这一速度。

## 感知器学习

与最近邻策略相反，解决分类问题的另一种方法是将数据视为一个整体并尝试创建决策边界。在二维数据中，我们可以在两种类型的观察之间画一条线。每个附加数据点将根据绘制的线的一侧进行分类。

![决策边界](https://cs50.harvard.edu/ai/2024/notes/4/decisionboundary.png)

这种方法的缺点是数据很混乱，而且很少有人能够毫无错误地画一条线并将类整齐地划分为两个观察值。通常，我们会妥协，划出一个边界，通常可以正确地分隔观察结果，但偶尔也会对它们进行错误分类。

在这种情况下，输入

+   $x_1$ = 湿度
+   $x_2$ = 压力

将被赋予一个假设函数$h(x_{1}, x_{2})$，该函数将输出当天是否会下雨的预测。它将通过检查观察结果落在决策边界的哪一侧来实现这一点。形式上，该函数将通过添加一个常数对每个输入进行加权，以以下形式的线性方程结束：

+   下雨  $w_0+w_{1}x_{1}+w_{2}x_{2} \geq 0$
+   否则  没有雨

通常，输出变量将被编码为 1 和 0，其中如果方程得出的结果大于 0，则输出为 1（有雨），否则为 0（无雨）。

权重和值由向量表示，向量是数字序列（可以存储在 Python 中的列表或元组中）。我们生成一个权重向量**w**: (w ₀, w ₁, w2)，获得最佳权重向量是机器学习算法的目标。我们还生成一个输入向量**X**: (1, x ₁, x2)。

我们取两个向量的点积。也就是说，我们将一个向量中的每个值乘以第二个向量中的相应值，得到上面的表达式：w ₀ + w ₁ x ₁ + w2x2. 输入向量中的第一个值是 1，因为当乘以权重向量 w ₀时，我们希望将其保持为常数。

因此，我们可以用以下方式表示我们的假设函数：

![点积方程](https://cs50.harvard.edu/ai/2024/notes/4/dotproduct.png)

由于算法的目标是找到最佳权重向量，因此当算法遇到新数据时，它会更新当前权重。它使用**感知器学习规则**来实现：

![感知器学习规则](https://cs50.harvard.edu/ai/2024/notes/4/perceptronlearning.png)

该规则的重要要点是，对于每个数据点，我们调整权重以使我们的函数更加准确。对于我们的观点来说，细节并不重要，每个权重都设置为等于自身加上括号中的某个值。这里，y 代表观测值，而假设函数代表估计值。如果它们相同，则整个项等于零，因此权重不变。如果我们低估了（在观察到有雨时称为“无雨”），则括号中的值将为 1，权重将增加 x ᵢ的值，并按学习系数α 缩放。如果我们高估了（在没有观察到雨的情况下称为 Rain），则括号中的值将为 -1，权重将减少 x 乘以α 的值。α越高，每个新事件对权重的影响越强。

此过程的结果是一个阈值函数，一旦估计值超过某个阈值，该函数就会从 0 切换到 1。

![硬阈值](https://cs50.harvard.edu/ai/2024/notes/4/hardthreshold.png)

这种类型的函数的问题是它无法表达不确定性，因为它只能等于 0 或 1. 它使用**硬阈值**。解决这个问题的一种方法是使用逻辑函数，它使用**软阈值**。逻辑函数可以产生 0 到 1 之间的实数，这将表达对估计的置信度。值越接近 1，下雨的可能性就越大。

![软阈值](https://cs50.harvard.edu/ai/2024/notes/4/softthreshold.png)

## 支持向量机

除了最近邻和线性回归之外，另一种分类方法是支持向量机。该方法在决策边界附近使用附加向量（支持向量）来在分离数据时做出最佳决策。考虑下面的例子。

![支持向量机](https://cs50.harvard.edu/ai/2024/notes/4/supportvector.png)

所有决策边界都起作用，因为它们将数据分开而没有任何错误。然而，它们同样好吗？最左边的两个决策边界非常接近一些观察结果。这意味着与一组仅略有不同的新数据点可能会被错误地分类为另一组。与此相反，最右边的决策边界与每个组保持最远的距离，从而为其中的变化提供了最大的余地。这种类型的边界尽可能远离它所分隔的两个组，称为**最大边距分隔符**。

支持向量机的另一个好处是它们可以表示二维以上的决策边界，以及非线性决策边界，如下所示。

![圆决策边界](https://cs50.harvard.edu/ai/2024/notes/4/circleboundary.png)

总而言之，解决分类问题有多种方法，没有一种方法总是比另一种更好。每种方法都有其缺点，并且在特定情况下可能比其他方法更有用。

## 回归

回归是函数的监督学习任务，它将输入点映射到连续值（某个实数）。这与分类不同，因为分类问题将输入映射到离散值（有雨或无雨）。

例如，公司可能会使用回归来回答广告支出如何预测销售收入的问题。在这种情况下，观察到的函数 *f（广告）* 表示在广告上花费了一些钱后观察到的收入（请注意，该函数可以采用多个输入变量）。这些是我们开始的数据。有了这些数据，我们想要提出一个假设函数*h（广告）*，它将尝试近似函数*F*的行为。 *H*将生成一条线，其目标不是区分观察类型，而是根据输入预测输出值。

![Regression](https://cs50.harvard.edu/ai/2024/notes/4/regression.png)

## 损失函数

损失函数是一种量化上述任何决策规则所损失的效用的方法。预测越不准确，损失就越大。

对于分类问题，我们可以使用**0-1 损失函数**。

+   *我*（实际，预测）：
    +   如果实际 = 预测则为 0
    +   1 否则

换句话说，当预测不正确时，该函数会获得价值；而当预测正确时（即，当观测值和预测值匹配时），该函数不会获得价值。

![0-1 Loss Function](https://cs50.harvard.edu/ai/2024/notes/4/01loss.png)

在上面的示例中，值为 0 的日期是我们正确预测天气的日期（下雨天位于线下方，非下雨天位于线上方）。然而，线以下不下雨的日子和线以上确实下雨的日子是我们无法预测的。我们给每个值 1 并将它们相加，以获得我们的决策边界有损程度的经验估计。

预测连续值时可以使用 L ₁和 L2 损失函数。在这种情况下，我们有兴趣量化每个预测*多少*它与观察值的不同。我们通过取观测值减去预测值的绝对值或平方值（即预测与观测值的距离）来实现这一点。

+   $L_{1}$：*L*(实际，预测) = | 实际 - 预测 |
+   $L_2$：*L*(实际，预测) =（实际 - 预测）²

人们可以选择最适合其目标的损失函数。$L_{2}$ 对异常值的惩罚比 $L_{1}$更严厉，因为它对差异进行平方。$L_1$可以通过将回归线上每个观测点到预测点的距离相加来可视化：

![L₁](https://cs50.harvard.edu/ai/2024/notes/4/l1.png)

## 过拟合

过度拟合是指模型非常适合训练数据，以至于无法推广到其他数据集。从这个意义上说，损失函数是一把双刃剑。在下面的两个示例中，损失函数被最小化，使得损失等于 0. 但是，它不太可能很好地拟合新数据。

![过拟合](https://cs50.harvard.edu/ai/2024/notes/4/overfitting.png)

例如，在左图中，屏幕底部红色点旁边的点很可能是雨（蓝色）。然而，对于过度拟合的模型，它将被分类为“无雨”（红色）。

## 正则化

正则化是惩罚更复杂的假设以支持更简单、更一般的假设的过程。我们使用正则化来避免过度拟合。

在正则化中，我们通过将假设函数 h 的损失和复杂性的度量相加来估计假设函数 h 的成本。

*成本*(h) = *损失*(h) + λ*复杂*(h)

Lambda (λ) 是一个常数，我们可以用它来调节对成本函数的复杂性进行惩罚的强度。λ越高，复杂度越高。

测试模型是否过度拟合的一种方法是使用**坚持交叉验证**。在这种技术中，我们将所有数据分成两部分：**训练集**和**测试集**。我们在训练集上运行学习算法，然后看看它对测试集中数据的预测效果如何。这里的想法是，通过测试训练中未使用的数据，我们可以衡量学习的泛化程度。

保留交叉验证的缺点是我们无法在一半的数据上训练模型，因为它用于评估目的。解决这个问题的一种方法是使用***k*\-折叠交叉验证**。在此过程中，我们将数据分为 k 组。我们运行训练 k 次，每次都留下一个数据集并将其用作测试集。我们最终对我们的模型进行了 k 个不同的评估，我们可以对这些评估进行平均，并在不丢失任何数据的情况下估计我们的模型如何泛化。

## scikit 学习

与 Python 的情况一样，有多个库可以让我们方便地使用机器学习算法。scikit-learn 就是此类库之一。

作为示例，我们将使用假钞的[CSV](https://en.wikipedia.org/wiki/Comma-separated_values)数据集。

![Banknotes](https://cs50.harvard.edu/ai/2024/notes/4/banknotes.png)

左边四列是我们可以用来预测纸币真假的数据，这是人类提供的外部数据，编码为 0 和 1. 现在我们可以在这个数据集上训练我们的模型，看看是否可以预测新钞票是否真品。

```auto
import csv
import random

from sklearn import svm
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# model = KNeighborsClassifier(n_neighbors=1)
# model = svm.SVC()
model = Perceptron()
```

请注意，导入库后，我们可以选择要使用的模型。其余代码将保持不变。SVC 代表支持向量分类器（我们称为支持向量机）。KNeighborsClassifier 使用 k 邻居策略，并要求输入应考虑的邻居数量。

```auto
# Read data in from file
with open("banknotes.csv") as f:
    reader = csv.reader(f)
    next(reader)

    data = []
    for row in reader:
        data.append({
            "evidence": [float(cell) for cell in row[:4]],
            "label": "Authentic" if row[4] == "0" else "Counterfeit"
        })

# Separate data into training and testing groups
holdout = int(0.40 * len(data))
random.shuffle(data)
testing = data[:holdout]
training = data[holdout:]

# Train model on training set
X_training = [row["evidence"] for row in training]
y_training = [row["label"] for row in training]
model.fit(X_training, y_training)

# Make predictions on the testing set
X_testing = [row["evidence"] for row in testing]
y_testing = [row["label"] for row in testing]
predictions = model.predict(X_testing)

# Compute how well we performed
correct = 0
incorrect = 0
total = 0
for actual, predicted in zip(y_testing, predictions):
    total += 1
    if actual == predicted:
        correct += 1
    else:
        incorrect += 1

# Print results
print(f"Results for model {type(model).__name__}")
print(f"Correct: {correct}")
print(f"Incorrect: {incorrect}")
print(f"Accuracy: {100 * correct / total:.2f}%")
```

运行算法的手动版本可以在本讲座的源代码中找到，位于 banknotes0.py 下。由于算法经常以类似的方式使用，scikit-learn 包含了额外的函数，使代码更加简洁和易于使用，这个版本可以在 banknotes1.py 下找到。

## 强化学习

强化学习是机器学习的另一种方法，在每次行动之后，代理都会以奖励或惩罚（正数值或负数值）的形式获得反馈。

![强化学习](https://cs50.harvard.edu/ai/2024/notes/4/reinforcement.png)

学习过程从环境向代理提供状态开始。然后，代理对状态执行操作。基于此操作，环境将向智能体返回一个状态和奖励，其中奖励可以是正的，使该行为在未来更有可能发生，也可以是负的（即惩罚），使该行为在未来发生的可能性较小。

例如，这种类型的算法可用于训练步行机器人，其中每一步返回一个正数（奖励），每次跌倒返回一个负数（惩罚）。

## 马尔可夫决策过程

强化学习可以被视为马尔可夫决策过程，具有以下属性：

+   状态集***S***
+   一组操作***动作 (S)***
+   转换模型***P(s’| s, a)***
+   奖励函数***R(s, a, s’)***

例如，考虑以下任务：

![马尔可夫决策过程演示](https://cs50.harvard.edu/ai/2024/notes/4/markov.png)

代理是黄色圆圈，它需要到达绿色方块，同时避开红色方块。任务中的每个方块都是一个状态。向上、向下或向两侧移动都是一个动作。转换模型为我们提供执行动作后的新状态，奖励函数是代理获得什么样的反馈。例如，如果智能体选择向右走，它将踩到红色方块并得到负面反馈。这意味着智能体将了解到，当处于左下方块的状态时，它应该避免向右走。这样，代理将开始探索空间，学习应该避免哪些状态-动作对。该算法可以是概率性的，根据奖励增加或减少的概率选择在不同状态下采取不同的行动。当智能体到达绿色方块时，它将获得正奖励，得知采取在之前状态下采取的行动是有利的。

## Q-学习

Q-Learning 是强化学习的一种模型，其中函数***Q(s, a)***输出在状态 。

模型开始时所有估计值都等于 0（对于所有*, 一个*，***Q(s,a)* = 0**）。当采取行动并收到奖励时，该函数会执行两件事：1) 根据当前奖励和预期的未来奖励估计***Q(s, a)***的值，2) 更新***Q(s, a)***考虑旧的估计和新的估计。这为我们提供了一种算法，能够改进过去的知识，而无需从头开始。

***Q(s, a) ⟵ Q(s, a) + α (新值估计 - Q(s, a))***

***Q(s, a)***的更新值等于***Q(s, a)***的先前值以及一些更新值。该值被确定为新值和旧值之间的差值乘以学习系数α。当α = 1 时，新的估计值会覆盖旧的估计值。当α = 0 时，估计值永远不会更新。通过提高和降低α，我们可以确定新估计更新先前知识的速度。

新的价值估计可以表示为奖励（r）与未来奖励估计之和。为了获得未来的奖励估计，我们考虑采取最后一个动作后获得的新状态，并添加该新状态下将带来最高奖励的动作的估计。这样，我们不仅可以通过收到的奖励来估计在状态*s*下执行操作*A*的效用，还可以通过下一步的预期效用来估计。未来奖励估计的值有时会与系数伽马一起出现，该系数控制着未来奖励的估值。我们最终得到以下等式：

![Q 学习公式](https://cs50.harvard.edu/ai/2024/notes/4/qlearning.png)

**贪婪决策**算法完全折扣未来的估计奖励，而不是始终选择当前状态***s***中具有最高 。

这让我们讨论**探索与利用**的权衡。贪心算法总是会利用已经建立的行动来带来好的结果。然而，它总是遵循相同的路径来解决问题，而永远不会找到更好的路径。另一方面，探索意味着算法可以在到达目标的途中使用以前未探索过的路线，从而使其能够沿途发现更有效的解决方案。例如，如果你每次都听相同的歌曲，你知道自己会喜欢它们，但你永远不会了解你可能更喜欢的新歌曲！

为了实现探索和利用的概念，我们可以使用**ε (epsilon) 贪婪**算法。在这种类型的算法中，我们将ε 设置为我们想要随机移动的频率。该算法以 1-ε的概率选择最佳移动（利用）。该算法以概率ε 选择随机移动（探索）。

训练强化学习模型的另一种方法是在整个过程结束时而不是在每次移动时提供反馈。例如，考虑 Nim 游戏。在这个游戏中，不同数量的物体分布在堆之间。每个玩家从任意一堆中拿走任意数量的物体，拿走最后一个物体的玩家就输了。在这样的游戏中，未经训练的 AI 会随机下棋，很容易战胜它。为了训练 AI，它会从随机玩游戏开始，最后获胜奖励为 1，失败奖励为-1. 例如，当它经过 10,000 场比赛的训练时，它已经足够聪明，很难战胜它。

当游戏具有多个状态和可能的动作（例如国际象棋）时，这种方法对计算的要求就更高。为每种可能状态下的每种可能的移动生成估计值是不可行的。在这种情况下，我们可以使用**函数逼近**，它允许我们使用各种其他特征来近似***Q(s, a)***，而不是为每个状态-动作对存储一个值。因此，算法能够识别哪些动作足够相似，因此它们的估计值也应该相似，并在决策中使用这种启发式方法。

## 无监督学习

在我们之前看到的所有情况下，例如在监督学习中，我们都有带有标签的数据，算法可以从中学习。例如，当我们训练识别假钞的算法时，每张钞票都有四个具有不同值（输入数据）以及是否是假钞（标签）的变量。在无监督学习中，仅存在输入数据，并且人工智能学习这些数据中的模式。

**聚类**

聚类是一种无监督学习任务，它获取输入数据并将其组织成组，以便相似的对象最终位于同一组中。例如，这可以用于遗传学研究中，当试图找到相似的基因时，或者在图像分割中，当根据像素之间的相似性定义图像的不同部分时。

## k-均值聚类

k-means 聚类是一种执行聚类任务的算法。它将所有数据点映射到一个空间中，然后在该空间中随机放置 k 个聚类中心（由程序员决定有多少个；这是我们在左侧看到的起始状态）。每个聚类中心只是空间中的一个点。然后，每个簇被分配所有距离其中心比距离其他中心最近的点（这是中间的图片）。然后，在迭代过程中，聚类中心移动到所有这些点的中间（右侧的状态），然后再次将点重新分配给中心现在最接近它们的聚类。当重复该过程后，每个点仍保持在与之前相同的簇中时，我们就达到了平衡，算法结束，留下了在簇之间划分的点。

![k-均值聚类](https://cs50.harvard.edu/ai/2024/notes/4/kclustering.png)