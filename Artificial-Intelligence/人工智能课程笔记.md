# 机器学习(Machine Learning)
---
> Improve on *T*ask with respect to *P*erformance metric based on *E*xperience

- 主要从效果(Effectiveness)和效率(efficiency of the computation)两方面衡量

机器学习的必要性：未知环境和复杂系统，大规模数据挖掘，促进人类智能

**Objectiveness**和**Optimization**

## 监督学习
> [!SUMMARY] 基本思想
> 给定$x$和$f(x)$，试图求解出一个假设的(*hypothesis*)映射函数$x$逼近$f$，其主要任务如下：
> 1. 如何定义函数形式
> 2. 如何优化函数参数和函数结构

- Mean Square Error(MSE)：均值平方根误差

Ockham's Razor(奥卡姆剃刀原理)：在逼近数据的前提下令函数尽可能简单，使函数具有普适性

### 决策树(Decision Tree)
选择作为根节点的特征：以某个特征能将子节点分成几个独立性更强的部分

- Entropy(熵)：从函数**结果**进行计算$$I(\frac p{p+n},\frac n{p+n})=-\frac p{p+n}\log_2\frac p{p+n}-\frac n{p+n}\log_2\frac n{p+n}$$熵越大表示区分数据越混乱，越小表示越独立
- Remainder Entropy(剩余熵)：$$R(A) = \sum_{i=1}^v\frac{p_i+n_i}{p+n}I(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i})$$
- 信息增益(Information Gain)：选择`IG`最大的特征作为根节点特征 $$IG(A) = I\left(\frac p{p+n},\frac n{p+n}\right)- R(A)$$

#### 预防过拟合(Overfitting)
1. 扩大数据集
2. 提前停止拟合
3. 允许在数据集上过拟合，但随后对模型做剪枝
   - 通过做DL(Description Length)分析，选取Code Length更低的决策树

### 贝叶斯推论
> [!NOTE] 贝叶斯定理
> $$P\left(h|D\right)=\frac{P\left(D,h\right)}{P\left(D\right)}=\frac{P\left(D|h\right)P\left(h\right)}{P\left(D\right)}$$其中$D$表示数据，$h$表示假设；$P(h|D)$表示后验概率(Posterior Probability)，$P(h)$表示先验概率(Prior Probability)，$P(D|h)$表示类条件概率(Class-conditional Probability)

- MAP(Maximum A Posterior)：最大后验估计，通过计算每个候选假设的后验概率找到最大项

- 简化类条件概率$P(D|h)$的形式方法：
  1. 朴素贝叶斯分类器(Naive Bayesian Classifiers, NBC)：假设各分量间没有关系，无需考虑协方差，每个分量都只有均值和方差需要考虑
  2. 贝叶斯信念网络(Bayesian Belief Classification, BBC)：将有关系的分量建立图模型联系
  3. 高斯混合模型(Gaussian Miature Model, GMM)：用简单的函数叠加形成复杂函数

#### 朴素贝叶斯分类器(NBC)
$$P\Big(D\Big|h\Big)=P\Big(d_1,\cdots,d_n\Big|h\Big)=\prod_iP\Big(d_i\Big|h\Big)$$即可以只看做各个特征概率之间的连乘

#### 贝叶斯信念网络(BBN)
在给定条件$Z$的时候假设特征$X$与$Y$相互独立，即$$P\Big(x_1,\cdots,x_l\Big|y_1,\cdots,y_m,z_1,\cdots,z_n\Big)=P\Big(x_1,\cdots,x_l\Big|z_1,\cdots,z_n\Big)$$

集合概率本质上是一个概率分布的表达形式，可以表示为$$P\left(x_1,\cdots,x_n\right)=\prod_{i=1}^nP\left(x_i\right|Parents\left(x_i\right)$$可以用有向图或条件概率表(Conditional Probability Table, CPT)表示![](Artificial-Intelligence/Pasted%20image%2020240602012513.png)
- 既能对概率分布做有效表达，又能限制其复杂性，使规模增长性由指数变为线性增长

- 编辑概率公式：$$P(x|y)=\alpha P(x,y) = \alpha\sum\limits_zP(x,y,z)$$其中$\alpha=P(y)^{-1}$，简单理解就是把$z$变量的所有取值都考虑一遍，这样相当于特征$z$不会影响到原有的情况。例如可以有$$\begin{align} P(B|j,m)&=\frac{P(B,j,m)}{P(j,m)}\\&=\alpha P(B,j,m)\\&=\alpha\sum\limits_{a}\sum\limits_{e}P(B,e,a,j,m) \end{align}$$

- 优化目标：极大似然估计(Maximum Likelihood Estimation, MLE)
- 优化方法：梯度上升$$\frac{\partial\ln P_w(D)}{\partial w_{ijk}}=\sum_{d\in D}\frac{P_w\left(x_{ij},u_{ik}|d\right)}{w_{ijk}}$$

## 无监督学习



# 人工神经网络(Artificial Neural Network)
---



# 搜索与问题求解(Search and Problem Solving)
---




# 知识与推理(Knowledge and Reasoning)
---



# 进化计算(Evolutionary Computation)
---



# 群体智能(Swarm Intelligence)
---




# 新型人工智能(Nouvelle AI)
---
