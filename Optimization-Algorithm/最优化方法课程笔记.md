**最优化问题：在一定约束条件下，如何选择变量从而最大化/最小化预设目标**

# 从复杂度角度衡量算法好坏
---
## 算法复杂度

 分为时间复杂度(计算所需的步数或指令条数)和空间复杂度(计算所需的存储空间大小)

### 时间复杂度
时间复杂度是关于问题规模$n$(或算法输入的大小)的函数，描述计算所需步长随$n$增长的速度；用$\mathrm{O}$表示，描述当$n$趋于无穷时，复杂度的增长情况![](Pasted%20image%2020240227192810.png)![](Pasted%20image%2020240227193017.png)

## P、NP、NP难问题

### P问题(Polynomial time)
找到一个在多项式时间内(包含多项式时间)解决它的算法，如计算多位数乘积等

### NP问题(Nondeterministic Polynomial)
能在多项式时间之内验证一个解是否正确的问题 **(注意并不是在多项式时间之内不能解决)**，如数独问题等

> 所有P问题都属于NP问题，反之至今未解决（没有证明对于任意NP问题都属于P问题）

### NP完全问题(NP-Complete)
所有NP问题可于多项式时间内归约到的NP问题，如旅行商问题的判定问题、0-1背包问题的判定问题、分团问题等

### NP难问题(NP-Hard)
所有NP问题可用于多项式时间内归约到的问题，如旅行商问题等


> [!TIPS] NP难问题与NP完全问题对比
> 1. NP完全问题是NP问题，NP难问题不用是NP问题
> 2. NP完全问题一般是判定类问题，NP难问题不用是判定类问题

![](Pasted%20image%2020240227201431.png)

# 最优化问题一般形式
> [!IMPORTANT] 三要素
> **目标函数、约束条件、决策变量**
> ![](Pasted%20image%2020240227211132.png)
> 1. 可行域(feasible region)有时可以划入约束条件，反之亦然
> 2. 最大化问题和带有$\leq$约束的问题都可以整理成一般形式

转化为一般形式可以方便判断最优化问题的类别(凸优化、线性规划、二次规划等)，同时也便于选择求解方法和实施算法

## 常用概念
1. 可行解(feasiable solusion)：满足约束条件和可行域的解
2. 最优解(optimal solusion)：令目标函数达到最小值的可行解
3. 全局最优解(global optimal solusion)：对所有可行解$x$，有$f(x^*)\leq f(x)$
4. 局部最优解(local optimal solusion)：存在$\delta>0$，使对所有满足$\|x-x^*\|<\delta$的可行解$x$，有$f(x^*)\leq f(x)$

# 无约束凸优化问题
> [!TITLE] 定义
> $$\begin{aligned}&\min\quad f(x)\\&\text{var.}\quad\mathcal{X}\in\mathcal{R}^n\end{aligned}$$
> 其中$f(x)$是二阶连续的可微凸函数

## 凸集(convex set)
> [!NOTE] 定义
> 若对$\forall x,y \in \mathbb C$及$\lambda \in [0,1]$，有$\lambda x + (1-\lambda)y \in \mathbb C$，则集合$\mathbb C$为凸集

## 凸函数
> [!NOTE] 定义
> 1. 定义域$\mathbb C$是凸集
> 2. 对$\forall x,y \in \mathbb C$及$\lambda \in [0,1]$，$f(\cdot)$满足$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$，则集合函数$f(x)$为凸函数
> 
> 严格凸函数：$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$

### 凸函数的判定

- （凸函数的一阶条件）若$f(x)$是定义在凸集$\mathbb C$上的一阶可微函数，那么$f(x)$是凸函数的充要条件为：$$\text{对}\forall x,y\in \mathbb C，\text{有}f(y)\geq f(x)+\nabla f(x)^T(y-x)$$
- **(常用定理)**（凸函数的二阶条件）若$f(x)$是定义在凸集$\mathbb C$上的二阶连续可微函数，那么$f(x)$是凸函数的充要条件为：$$\text{对}\forall x,y\in \mathbb C，f(x)\text{的海森矩阵是半正定矩阵，即}\nabla^{2}f(x)\succcurlyeq0$$

### 海森矩阵($\nabla^{2} f(x)$)

$$\left.\nabla^{2}f(\boldsymbol{x})=\left[\begin{matrix}{\frac{\partial^{2}f}{\partial x_{1}^{2}}}&{\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}}&{\cdots}&{\frac{\partial^{2}f}{\partial x_{1}\partial x_{n}}}\\\\{\frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}}&{\frac{\partial^{2}f}{\partial x_{2}^{2}}}&{\cdots}&{\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}}\\\\\vdots&\vdots&\ddots&\vdots\\\\{\frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}}&{\frac{\partial^{2}f}{\partial x_{n}\partial x_{2}}}&{\cdots}&{\frac{\partial^{2}f}{\partial x_{n}^{2}}}\\\end{matrix}\right.\right]$$

其中对$f(x_{1}, x_{2})$有$$\nabla^2f(x)=\begin{bmatrix}\frac{\partial^2f(x_1,x_2)}{\partial x_1^2}&\frac{\partial^2f(x_1,x_2)}{\partial x_1\partial x_2}\\\frac{\partial^2f(x_1,x_2)}{\partial x_2\partial x_1}&\frac{\partial^2f(x_1,x_2)}{\partial x_2^2}\end{bmatrix}$$
### 半正定矩阵

对$n\times n$实对称矩阵$\mathbb A$，若对任意向量$u\in \mathbb{R}^n$，有$u^{T}Au \geq0$；或矩阵$\mathbb A$的所有顺序主子式（$i$阶对角矩阵构成的行列式）都为非负，则$\mathbb A$为半正定矩阵![](Pasted%20image%2020240227221437.png)

## 无约束凸优化问题

1. 若$f(x)$是凸函数，那么任何使$f(x)$达到局部极小值的局部最优解$x^{local}$也是使$f(x)$达到全局最小值的全局最优解
2. 若$f(x)$是⼀阶可微凸函数，那么$x^{*}$是最小化$f(x)$的全局最优解的充要条件为：$$\nabla f(x^{*})=0$$

### 线性回归
在实际线性回归中，还需要包含常数项$w_{0}$，同时要对$a_{11},a_{21},\cdots$做归一化处理

#### L1正则化
- 添加的正则项为$\|w\|=|w_{1}|+|w_2|+\cdots+|w_N|$，目标为希望出现更多取值为0的系数(LASSO)：$$\begin{aligned}\min\quad&\frac1M\|\mathbf{A}\boldsymbol{w}-\boldsymbol{b}\|_2^2+\lambda\|\boldsymbol{w}\|\\\mathrm{var}.\quad&\boldsymbol{w}\in\mathcal{R}^N.\end{aligned}$$

#### L2正则化
- 添加的正则项为$\|w\|_2^2=w_1^2+w_2^2+\cdots+w_N^2$：$$\begin{aligned}\min\quad&\frac1M\|\mathbf{A}\boldsymbol{w}-\boldsymbol{b}\|_2^2+\lambda\|\boldsymbol{w}\|_{2}^{2}\\\mathrm{var}.\quad&\boldsymbol{w}\in\mathcal{R}^N.\end{aligned}$$

# 梯度下降法
---
> [!NOTE] 梯度下降法的一般形式
> 1. 初始化当前解$X^{0}$及$t=0$
> 2. 确定下降方向为$\mathrm{d}^{t}=-\nabla f(x^t)$(向量)
> 3. 确定步长$\alpha^{t}>0$(标量)
> 4. 更新当前解：$x^{t+1}=x^{t}+\alpha^{t}\mathrm{d}^{t}$
> 5. 若满足停止条件则结束(通常采用$\|\nabla f(x^{t+1})\|_2$小于某预设阈值)，否则令$t\leftarrow t+1$且返回1

- 为确保$\mathrm d^{t}$是函数值下降的方向，$\mathrm d^t$应满足：$$\left(\nabla f(x^t)\right)^Td^t<0$$当梯度下降为一维时可化简为$f^{\prime}(x^t)d^t<0$，即$d^{t}$与$f^{\prime}(x^{t})$异号

## 梯度下降法的收敛性
> [!TIPS] 定义
> 设$f(x)$是一阶连续可微凸函数且梯度满足对所有$x,y\in \mathcal R^{n}$均有：$\|\nabla f(x)-\nabla f(y)\|_2\leq L\|x-y\|_2$(利普西茨连续Lipschitz continuity)
> 
> 若$x^{*}$是最小化$f(x)$的全局最优解，当$\alpha^t=\alpha\in\left(0,\frac1L\right]$，在梯度下降法下有$$f(x^t)-f(x^*)\leq\frac1{2t\alpha}\|x^0-x^*\|_2^2$$

# 牛顿法
---
> [!NOTE] 定义
> 牛顿法主要通过改进算法从⽽改善收敛速度慢的问题，利用函数的二阶泰勒展开式做近似，再确定$\mathrm d^{t}$
> 
> 1. 初始化当前解$X^{0}$及$t=0$
> 2. 确定下降方向为$d^t=-(\nabla^2f(x^t))^{-1}\nabla f(x^t)$(向量)
> 3. 更新当前解：$x^{t+1}=x^{t}+\alpha^{t}\mathrm{d}^{t}$
> 4. 若满足停止条件则结束(通常采用$\|\nabla f(x^{t+1})\|_2$小于某预设阈值)，否则令$t\leftarrow t+1$且返回1

- 优点：收敛速度很快
- 缺点：初始解$x^{0}$需要离$x^{*}$足够近(即仅有局部收敛性)；需要$f(\cdot)$二阶连续可微且$\left(\nabla^2f(x^t)\right)^{-1}$存在($\nabla^2f(x^t)$是正定矩阵)

**因此可以先使用梯度下降得到低精度解后再使用牛顿法**

# 线性规划问题
---
> [!NOTE] 定义
> 当目标函数和所有约束（包括$\mathcal D$对$\mathcal x$的约束）都是线性时，称为线性规划问题
> 
> - 一般形式(注意$\mathrm a$是向量)：$$f(x)=\mathrm{a}_{0}^{T}x+b_{0}$$
> - 标准形式：$$\begin{aligned}&\min&&\boldsymbol{c}^T\boldsymbol{x}\\&\mathrm{s.t.}&&\mathbf{A}\boldsymbol{x}=\boldsymbol{b}\\&\mathrm{var.}&&\boldsymbol{x}\geq\boldsymbol{0}\end{aligned}$$

1. 当我们需要将不等式约束变成等式约束时，可以通过构建松弛变量(slack variable)：![](Pasted%20image%2020240304211916.png)
2. 将决策变量的定义域变为正实数集，将相应变量改写成两个新的非负变量(人工变量Artificial variable)的差值：![](Pasted%20image%2020240304215247.png)

## 单纯形法
- 核心思想：按目标函数减小的方向，沿一系列对应相邻顶点的基可行解搜索

1. 对任何线性规划问题，最优解若存在，可行域的**顶点**中⼀定有最优解
2. 基可行解(图解法的相交顶点)与顶点对应
   - 对于标准线性规划，当有$m$个独立等式和$n$个变量($m\leq n$)，将其中$n-m$个变量设为0，计算剩余$m$个变量取值，可以得到共$\frac{n!}{m!(n-m)!}$个解，剔除非可行解即可得到所有**基可行解**(即所有顶点)
3. 按目标函数减小的⽅向，沿相邻顶点搜索，可以达到最优顶点（最优解）
4. 通常，两个恰好相差⼀个非零变量的基可行解对应两个相邻的顶点

### 任意线性规划的求解
1. 检查是否已经整理成标准形式
2. 找到一个基可行解(基可行解一定有$m$个变量非零，$n-m$个变量为0)
   - 旋转：每个等式只对应一个非零变量，且在该等式中非零变量系数为1
3. 计算该基可行解对应的目标函数值
   - 利用旋转方程将目标函数改写成关于零变量的函数
4. 找到进一步减小目标函数值的相邻基可行解
5. 重复搜索相邻基可⾏解的过程，直至找到最优解

### 单纯形法的一般步骤
1. 检查是否已经整理成标准形式
2. 找到初始基可⾏解
3. 围绕⾮零变量旋转，改写⽬标函数
   1. 通过改写后的⽬标函数，在（函数中系数为负的）当前零变量中选择⼀个，变为⾮零变量
   2. 在当前非零变量中选择一个，变为零变量
   3. 围绕非零变量旋转，改写目标函数
   4. 若改写后的⽬标函数中，所有变量系数⾮负，结束算法、输出最优解；否则，返回3.1


# 智能优化算法
---
## 模拟退火算法（Simulated Annealing, SA）
登山搜索只考虑最优化目标函数的方向，易陷入局部最优；简单引入随机性可能会导致抵达全局最优却跳出的现象

> [!NOTE] 模拟退火解决方法
> 在引入随机性的同时，随迭代次数变化动态调整随机性
> 
> 模仿物理退火，根据温度的变化调整用邻域解代替当前解的概率
> 1. 温度高随机性高：更接纳邻域解，即便质量较差
> 2. 温度低随机性低：趋向于只接纳质量较好的邻域解
> 3. 温度将为0时，停止搜索结束算法

### 模拟退火算法（求解最大化问题）
1. 初始化当前解$x_{current}$
2. 开始循环($for~t=1~to~\infty$)：
   1. 根据温度下降规则和$t$，确定当前温度$T$
   2. 若$T=0$则终止算法，输出$x_{current}$
   3. 随机选择一个邻域解$x_{neighbor}$
   4. 计算$\Delta E=f(x_{neighbor})-f(x_{current})$
   5. 若$\Delta E>0$，则令$x_{current}\leftarrow x_{neighbor}$
   6. 若$\Delta E\leq0$，则以$e^{\frac{\Delta E}{T}}$的概率令$x_{current}\leftarrow x_{neighbor}$

- 需要人为设置的变量：
  1. **起始温度**：需要设置的足够高，使得所有*比当前解差的邻域解*都能以一个足够高的概率被接纳
  2. **降温过程**：需要缓慢下降，常见几何递减：$T_{k}=T_{0}\alpha^{k}$($\alpha$一般取0.8到0.99之间)
  3. **结束条件**：连续不接纳邻域解的次数超过限制，最近若干解的质量平均值几乎不再上升等

## 遗传算法（Genetic Algorithm，GA）
1. 初始化当前种群$P(t)$
2. 评估当前种群$P(t)$
3. 开始循环($for~t=1~to~\infty$)：
   1. 根据当前种群$P(t)$，确定父代种群$P_{P}(t)$
   2. 根据父代种群$P_{P}(t)$，通过基因交叉生成子代种群$P_{C}(t)$
   3. 对子代种群$P_{C}(t)$进行基因突变
   4. 评估子代种群$P_{C}(t)$
   5. 根据当前种群$P(t)$和子代种群$P_{C}(t)$，筛选出新的当前种群$P(t+1)$
   6. 检查循环终止条件，若满足则结束算法