# 卷积GCN学习
<small>基于[一文读懂图卷积GCN](https://zhuanlan.zhihu.com/p/89503068)文章进行的学习</small>
## 图神经网络的分类
1. Graph Embedding（图嵌入）
   - 图嵌入(GE)，通常有两个层次含义：
     1. **将图中的节点表示成低维、实值、稠密的向量形式**：使得得到的向量形式可以在向量空间中具有表示以及推理的能力，这样的向量可以用于下游的具体任务中。例如用户社交网络得到节点表示就是每个用户的表示向量，再用于节点分类等
     2. **将整个图表示成低维、实值、稠密的向量形式**：用来对整个图结构进行分类
   - 图嵌入的方式主要有基于矩阵分解、DeepWalk和GNN三种：
     1. **矩阵分解**：将节点间的关系用矩阵的形式加以表达，然后分解该矩阵以得到嵌入向量。常用矩阵形式有邻接矩阵、拉普拉斯矩阵、节点概率转移矩阵等
     2. **DeepWalk**：DeepWalk 是基于 word2vec 词向量提出的。word2vec 在训练词向量时，将语料作为输入数据，而图嵌入输入的是整张图，两者看似没有任何关联。但是 DeepWalk 的作者发现，预料中词语出现的次数与在图上随机游走节点被访问到底的次数都服从幂律分布。因此 DeepWalk 把节点当做单词，把随机游走得到的节点序列当做句子，然后将其直接作为 word2vec 的输入可以节点的嵌入表示，同时利用节点的嵌入表示作为下游任务的初始化参数可以很好的优化下游任务的效果。
     3. **Graph Neural Network**：见下一小节
2. Graph Neural NetWork
   - 图神经网络是神经网络在图上应用的模型统称，按照传播方式不同也可以分为不同的类型，如图卷积神经网络（GCN）、图注意力网络（GAT）等
3. Graph Convolutional Network
   - 图卷积神经网络是一类采用图卷积的神经网络
   
综上，三个概念关系如下：

**图卷积神经网络GCN属于图神经网络GNN的一类，是采用卷积操作的图神经网络，可以应用于图嵌入GE。**
## 卷积 vs 图卷积
如图所示，数字图像就是一个二维的离散信号，对数字图像做卷积操作其实就是利用卷积核在图像上滑动，将图像点上的像素灰度值与对应的卷积核上的数值相乘，然后将所有相乘后的值相加作为卷积核中间像素对应的图像上像素的灰度值，并最终滑动完所有图像的过程。![[Pasted image 20231214162048.png]]
<big>**用随机的共享的卷积核得到像素点的加权和从而提取到某种特定的特征，然后用反向传播来优化卷积核参数就可以自动的提取特征，是CNN特征提取的基石**</big>

然而，生活中很多数据集以图的形式进行存储，这些图网络信息并不像图像一样可以转化成整齐的矩阵排列，通常只是一些非结构化信息。为了像图像领域卷积一样，找到一个通用范式来进行图特征的抽取，提出了图卷积在图神经网络的应用。

对于大多数图模型，有一种类似通式的存在，这些模型统称GCNs。因此可以说，图卷积是处理非结构化数据的大利器，随着这方面研究的逐步深入，人类对知识领域的处理必将不再局限于结构化数据（ CV，NLP），会有更多的目光转向这一存在范围更加广泛，涵盖意义更为丰富的知识领域。

## 对图模型范式的拆解

### 图的定义
对于图，我们有以下特征定义：

对于图$G=(V, E)$，$V$为节点的集合，$E$为边的集合，对于每个节点$i$，均有其特征$x_i$，可以用矩阵$X_{N*D}$表示。其中$N$表示节点数，$D$表示节点特征数，也就是特征向量的维度

### 图卷积的形象化理解
可以类比一下word2vec中的邻域影响思想，即当前节点（中心词向量）的会受到周围的点（周围词向量）的影响，距离（关系）越近所受到的影响就越大

### 图相关矩阵的定义
因此，我们可以开始用图论中的度矩阵、邻接矩阵和拉普拉斯矩阵来度量节点的邻居节点关系。对于一个简单的例子，**度矩阵**$D$只有对角线有值，表示值为n的节点与多少个节点相连；**邻接矩阵**$A$中第$i$行$j$列为1的值表示值为$i$和$j$的两个点相关联，为0则表示两节点不相关联（在添加关联权重后可以进一步完善矩阵）；**拉普拉斯矩阵**$L$为$D-A$（这只是最简单的一种拉普拉斯矩阵）

### 图卷积的通式
任何一个图卷积层都可以写成一个非线性函数：
$$H^{l+1}=f(H^{l}, A)$$
$H^0=X$为第一层的输入，$X\in R^{N*D}$，$N$为图的节点个数，$D$为每个节点特征向量的维度，$A$为上面提到的邻接矩阵，不同模型的差异点在于函数$f$的实现不同

下面是几种具体的实现，但都统称为拉普拉斯矩阵
#### 拉普拉斯矩阵的实现方式
1. $$H^{l+1}=\sigma(AH^lW^l)$$其中，$W^l$为第$l$层的权重参数矩阵，$\sigma(·)$为线性激活函数，例如ReLU等
   
   这种思路就是基于上面图卷积的形象化理解一节中的思想，借助邻接矩阵$A$与特征$H$相乘，等价于某节点周围节点的特征相加。这样多层隐含层相加，能利用多层周围节点的信息
   
   但这种思路实际上存在两个问题：
   - 没有考虑的节点自身对节点的影响（类似于word2vec中中心词向量本身的影响）
   - 邻接矩阵$A$没有被归一化，导致在提取图特征时可能出现问题，即周围节点多的节点可能有较大的影响力
   
   因此接下来的两种实现分别针对这两个问题进行了优化
2. $$H^{l+1}=\sigma(LH^lW^l)$$这种方式引入拉普拉斯矩阵$L=D-A$的同时本身就引入了度矩阵$D$，从而解决了没有考虑节点自身信息自传递的问题
3. $$H^{l+1}=\sigma(D^{-\frac12}\hat{A}D^{-\frac12}H^lW^l)$$
   对于这里的拉普拉斯矩阵，有
   $$L^{sym}=D^{-\frac12}\hat{A}D^{-\frac12}=D^{-\frac12}(D-A)D^{-\frac12}=I_n-D^{-\frac12}AD^{-\frac12}$$
   这种方式通过对邻接矩阵两边乘以节点的度开方然后取逆，实现了对邻接矩阵的归一化操作。对于无向无权图，具体到每一个节点对$i,j$，矩阵中的元素由下式得出：
   $$L_{i,j}^\mathrm{sym}:=\begin{cases}1&\mathrm{~if~}i=j\mathrm{~and~}\deg(v_i)\neq0\\-\frac1{\sqrt{\deg(v_i)\deg(v_j)}}&\mathrm{~if~}i\neq j\mathrm{~and~}v_i\text{ is adjacent to }v_j\\0&\text{ otherwise}&\end{cases}$$
   其中$deg(v_i),deg(v_j)$为节点$i,j$的度，即度矩阵在$(i,j)$处的值
   
   对于为什么两边乘以一个矩阵的逆就完成了归一化的问题，以单个节点运算为例，做归一化就是除以它节点的度，这样每一条邻接边信息传递的值就被规范化了，不会因为某一个节点有10条边而另一个只有1条边导致前者的影响力比后者大，因为做完归一化前者的权重只有0.1了，从单个节点上升到二维矩阵的运算，就是对矩阵求逆了，乘以矩阵的逆的本质，就是做矩阵除法完成归一化。但左右分别乘以节点i,j度的开方，就是考虑一条边的两边的点的度。
   
#### 拉普拉斯的另一种表述方式
上文是以矩阵的形式来进行计算，接下来我们可以从节点的角度来进行计算。

在上文所述中，对于节点来说归一化操作就是除以节点的度，上升到矩阵来说就是乘以度矩阵的逆。因此，对于第$l+1$层的节点特征$h_{v_i}^{l+1}$，对于它的邻接节点$j\in N$，$N$是节点$i$所有周围节点的集合，可以通过下述公式得到$$h_{v_i}^{l+1}=\sigma(\sum_j\frac1{c_{ij}}h_{v_j}^lW^l)$$
其中，$c_{ij}=\sqrt{d_id_j}$，$j\in N_i$，$N_i$为$i$的周围节点，$d_{i},d_j$为$i,j$的度

# 超图学习
## 基本概念
在数学上，普通的图（Graph）是用来表示对象与对象之间关系的一种形式，普通图一般用一个二元组$G=<V,E>$来表示，$V$表示图的顶点集，$E$表示图的边集。然而，普通图一般用来描述存在二元关系的一组变量，即对象之间不存在多元关联关系，因此如果简单的把多元关系强制转换为二元关联关系，那么将会丢失很多有用的信息。

近年来，一种普通图的推广变体——超图（HyperGraph）被提出。超图与普通图的不同主要在于边上顶点的不同，在普通图中，一条边上只能包含两个顶点，而在超图中，边被称为**超边(HyperEdge)**，一条超边中可以包含多个顶点，如果所有的超边都只包含两个顶点，那么超图就会退化成普通图

与普通图的表示相似的是，我们也可以用一个三元组$G=<V,E,W>$来表示超图。其中，$V$代表超图的有限顶点集，$E$代表超图的超边集，$W$是超边的权重集。对于超边$e\in E$而言，超边的度为超边上包含顶点的数目，即$\delta(e)=|e|$，顶点的度为与某一顶点相连的超边的条数，即$d(v)=\sum_{\{s\in E|v\in V\}}w(e)$

类似于普通图的矩阵表示法，超图也可以通过定义一个$|V|\times|E|$维的点边关联矩阵$H$来表示。在关联矩阵中，如果一个顶点$v$在超边$e$上，则$h(v, e)=1$，反之$h(v,e)=0$。根据关联矩阵的定义，在超图中，点的阶和超边的阶(度)可以进一步表示为：$$d(v)=\sum_{e\in E}w(e){h}(v,e)$$$$\delta(e)=\sum_{v\in V}{h}(v,e)$$
在此基础上，我们就可以定义顶点对角度矩阵$D_v(D)$，超边对角度矩阵$D_e$，以及超边权重度矩阵$W$。因此，超图的邻接矩阵就可以表示为$\mathrm{A=HWH^T-D_v}$
## 超图分割
很多超图问题可以通过超图分割来解决，对于一个给定的超图$G=<V,E,W>$而言，超图分割指存在一条超图切（HyperGraph cut）将超图的顶点集分为子集$S$和它的补集$S^c$，这样我们就可以定义，如果一条超边同时与$S$和$S^c$中的顶点相关联，则这条超边被切割。因此对于子集$S$而言，我们可以将其超边边界定义为一个超边集：$$\delta S=\{e\in E|e\cap S\neq\emptyset,e\cap S^{c}\neq\emptyset\}$$正是这个超边集将超图的顶点集分割为$S$和$S^c$两个子集。接下来，我们可以定义$S$的容量$volS$为顶点集$S$中所有的顶点度之和，即$\mathrm{volS}=\sum_{v\in S}d(v)$，进而可以得到超边边界的容量$vol\delta S$可以表示为$$\mathrm{vol}\delta S=\sum_{e\in\delta S}w(e)\frac{|e\cap S||e\cap S^{c}|}{|\delta(e)|}$$其中分数项表示超边$e$在$S$和$S^c$中的顶点数量对整体超边度量$\delta(e)$（即超边包含的顶点数量）的贡献比例。显然，有$\mathrm{vol\partial S}=\mathrm{vol\partial S}^\mathrm{c}=\mathrm{vol(S,S^c)}$，因此我们可以把超边边界的体积看做两个顶点子集之间的连接紧密度，而顶点子集的体积可以看做是子集内部顶点之间的连接紧密程度。

类似于普通图的图切问题，我们同样试图找到一个超图切，这个超图切所划分的两个顶点子集之间的关系是稀疏的，而子集内部顶点的联系是紧密的。因此，超图分割问题就可以转化为这样一个优化问题：$$\arg\min_{\phi\neq S\subset V}C(S)=vol(S,S^{c})(\frac{1}{vol(S)}+\frac{1}{vol(S^{c})})$$相当于我们要通过调整公式右侧$S$与$S^c$两顶点集的容量之比，最小化代价函数$C(S)$，使得超图分割的代价最小。根据文献可知，超图切的最优化目标与一个瑞利商一致（关于瑞利商详见[[图神经网络学习#^77e0f0]]）。假设一个列向量$q$，其维度为超边中的顶点的个数，则其元素值为：$$\mathrm{q(v)}=\begin{cases}+\sqrt{vol(S^c)/vol(S)}&\mathrm{~v~\in S}\\-\sqrt{vol(S)/vol(S^c)}&\mathrm{~v~\in S^c}&\end{cases}$$由上式，超图切的优化目标就转化为：$$\arg\min_{\phi\neq S\subset V}C(S)=\frac{q^{T}Lq}{q^{T}\Lambda q}$$$\Lambda$是一个以$vol(S)$作为元素的对角矩阵，$L$是拉普拉斯矩阵在超图中的归一化表示，即$$\mathrm{L}=D_{v}-HWD_{e}^{-1}H^{T}$$其中，$H$是超图的点边关联矩阵，若顶点$v_i$在超边$e_j$上，则$H_{i,j}=1$，反之为0。前文假设的向量$q$被认为是超图的切向量，当将超图切割问题转化为矩阵形式的表达后，涉及到一个目标函数的最小化。在此情境下，最小化$q^T L q$与求解矩阵$L$的最小特征值对应的特征向量有关。

具体来说，首先考虑超图切割问题的优化目标函数$C(S)$，其中$L$是超图的拉普拉斯矩阵，$\Lambda$是对角矩阵。这个优化问题的目标是找到一个向量$q$，使得这个目标函数的值最小化。

现在考虑矩阵$L$的最小特征值和对应的特征向量。最小特征值是通过求解如下问题得到的：$$\lambda_{\text{min}} = \min_{x \neq 0} \frac{x^T L x}{x^T x}$$
当矩阵$L$的最小特征值对应的特征向量$x_{\text{min}}$满足$Lx_{\text{min}} = \lambda_{\text{min}} x_{\text{min}}$时，$x_{\text{min}}$也是使得$q^T L q$最小的向量之一。

将$x_{\text{min}}$视为$q$，我们可以发现，当$q$是矩阵$L$的最小特征值对应的特征向量时，$q^T L q = \lambda_{\text{min}} q^T q$

由于在最小特征值问题中，分子部分$x^T L x$将取得最小值$\lambda_{\text{min}} x^T x$，所以当$q$对应于$L$的最小特征值时，$q^T L q$取得最小值$\lambda_{\text{min}} q^T q$。因此，超图切割问题的优化目标可以转化为求解矩阵$L$最小特征值对应的特征向量，即最小化向量$q$的问题。从数学角度分析这个组合最优化问题，就是求解矩阵束$(L,\Lambda)$最小特征值对应的特征向量。根据文献可知，上述问题的简化解可以从求解下式的最小非负特征值对应的特征向量来得到：$$\Delta=\mathrm{I}-D_{v}^{-\frac{1}{2}}HWD_{e}^{-1}H^TD_{v}^{-\frac{1}{2}}$$在上式中，I是一个单位矩阵，上式的结果就是超图的标准化拉普拉斯矩阵。对于超图分割问题而言，可以进一步的将优化目标函数推导成标准化损失函数，如下所示：$$\begin{aligned}
\Omega(F,G)&=\begin{aligned}\frac{1}{2}\sum_{e\in E}\sum_{(u,v)\in e}\frac{w(e)}{\delta(e)}\left|\left|\frac{F_u}{\sqrt{d(u)}}-\frac{F_v}{\sqrt{d(v)}}\right|\right|^2\end{aligned}  \\
&=\frac{1}{2}\sum_{e\in E}\sum_{u,v\in V}\frac{w(e)h(u,e)h(v,e)}{\delta(e)}\left(\frac{(F_u)^2}{d(u)}-\frac{F_uF_v^T}{\sqrt{d(u)d(v)}}-\frac{F_vF_u^T}{\sqrt{d(u)d(v)}}+\frac{(F_v)^2}{d(v)}\right) \\
&=\sum_{e\in E}\sum_{u,v\in V}\frac{w(e)h(u,e)h(v,e)}{\delta(e)}(\frac{(F_u)^2}{d(u)}-\frac{F_uF_v^T}{\sqrt{d(u)d(v)}}) \\
&=\begin{aligned}\sum_{e\in E}\sum_{u\in V}\frac{w(e)h(u,e)(F_u)^2}{d(u)}\sum_{v\in V}\frac{h(v,e)}{\delta(e)}-\sum_{e\in E}\sum_{u,v\in V}\frac{F_uw(e)h(u,e)h(v,e)F_v^T}{\delta(e)\sqrt{d(u)d(v)}}\end{aligned} \\
&\begin{aligned}=\sum_{e\in E}(F_u)^2\sum_{u\in V}\frac{w(e)h(u,e)}{d(u)}-\sum_{e\in E}\sum_{u,v\in V}\frac{F_uw(e)h(u,e)h(v,e)F_v^T}{\delta(e)\sqrt{d(u)d(v)}}\end{aligned} \\
&=\sum_{e\in E}(F_u)^2-\sum_{e\in E}\sum_{u,v\in V}\frac{F_uw(e)h(u,e)h(v,e)F_v^T}{\delta(e)\sqrt{d(u)d(v)}} \\
&=\mathrm{Tr}(F^TF)-\mathrm{Tr}(F^TD_v^{-1/2}HWD_e^{-1}H^TD_v^{-1/2}F) \\
&=\mathrm{Tr}(F^T(I-D_v^{-1/2}HWD_e^{-1}H^TD_v^{-1/2})F) \\
&=\mathrm{Tr}(F^{T}L_{H}F),
\end{aligned}$$在推导过程中，$||·||$表示了两个特征向量$F_u$和$F_v$归一化后的差的范数，我们把$\delta(e)$、$d(v)$和$w(e)$的对角矩阵表示形式分别表示为$D_e$、$D_v$和$W$。$H_e$是超图的点边关联矩阵的一个子矩阵，表示顶点在超边$e$上的影响程度，$D_{v}^{-\frac{1}{2}}$是其逆的平方根，$Tr(·)$是矩阵的迹

最终，向量$F$就是超图切，也可以被看作是超图的嵌入
### 有关瑞利商

^77e0f0

瑞利商（Rayleigh quotient）是一个在线性代数和特征值问题中常见的概念。它是用来描述矩阵特征向量和特征值之间关系的一种数学表达式。

对于一个实对称矩阵$A$和一个非零向量$x$，瑞利商定义为：$$R(x) = \frac{x^T A x}{x^T x}$$
其中$x^T$表示向量$x$的转置，$x^T A x$表示向量$x$经过矩阵$A$变换后的投影长度的平方，$x^T x$则是向量$x$的长度的平方。

在特征值问题中，瑞利商可以用来估计矩阵$A$的特征值。最大化或最小化瑞利商对应着找到相应矩阵的最大特征值和最小特征值，以及对应的特征向量。