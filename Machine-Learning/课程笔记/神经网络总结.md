# 神经网络(Neural Networks)
> [!TIP] 整体结构
> `input layer` $\rightarrow$ `hidden layer` $\rightarrow$ `output layer`

> [!IMPORTANT] 神经网络层传递
> $$a_{j}^{[l]} = g(\vec w_{j}^{[l]}\cdot \vec a^{[l-1]} + b_{j}^{[l]})$$
> $\vec a^{[l-1]}$表示上一层的输出值
> 
> $\vec w_{j}^{[l]}, b_{j}^{[l]}$表示第$l$层中第$j$单元的参数
> 
> $a_{j}^{[l]}$表示第$l$层第$j$单元的激活值
> 
> $g$表示激活函数(activation function)，如`sigmoid`函数

## 前向传播(forward propagation)
> [!TIPS] tips
> 就是一种沿神经网络传递进行推理的方式，用于应用神经网络进行推理

## 神经网络的结构
- 创建一个隐藏层
  ```python
  x = np.array([[200.0, 17.0]])
  layer_1 = Dense(units=3, activation = 'sigmoid')
  a1 = layer_1(x)
  """
  >>> a1
  >>> tf.Tensor([[0.2 0.7 0.3]], shape=(1, 3), dtype=float32)
  """
  ```
  其中，`Dense`表示全连接致密层，`units`为该层中的单元数或输出维度，`activation`表示激活函数

## `Numpy`和`Tensorflow`中的数据类型
1. 矩阵表示(matrix)
   对于$\begin{bmatrix}0.1&0.2\\-3&-4\\-.5&-.6\\7&8\end{bmatrix}$，维数(`x.shape == (4, 2)`)，应用`Numpy`进行导入表示为：
   ```python
   x = np.array([[0.1, 0.2,],
			 [-3.0, -4.0,], 
			 [-0.5, -0.6,], 
			 [7.0, 8.0,]])
   ```

2. 向量表示(vector)
   `x = np.array([200, 17])`这种向量表示方式，其实也是`(1, 2)`维矩阵表示；但在`Tensorflow`中，我们更通常使用`x = np.array([[200, 17]])`这种矩阵方式来表示，以提高大规模运算的效率

3. 张量(tensor)表示
   使用`a1.numpy()`可以将张量变量转化为`Numpy`形式的向量

## 构建神经网络
1. 应用`Sequential`函数(顺序框架)
   `Sequential`函数可以将几层神经网络进行连接，使其直接构成一个神经网络而不用我们手动去传递层间激活数据，即
   ~~~python
   # 一种表示
   layer_1 = Dense(units=3, activation = 'sigmoid')
   layer_2 = Dense(units=1, activation = 'sigmoid')
   model = Sequential([layer_1, layer_2])
   # 更简洁的一种表示
   model = Sequential([
	   Dense(units=3, activation = 'sigmoid'), 
	   Dense(units=1, activation = 'sigmoid')])
   model.compile(...)
   model.fit(x, y)
   model.predict(x_test)
   ~~~

2. 数据正则化
   使用`norm_x = tf.keras.layers.Normalization(axis=)`可以对数据进行正则化处理，使数据平均值为0，标准差为1，具体操作如下：
   ~~~python
   norm_l = tf.keras.layers.Normalization(axis=-1)
   norm_l.adapt(X)  # learns mean, variance
   Xn = norm_l(X)
   ~~~

3. 获取和更新`layers`权重参数
   ~~~python
   w, b = model.get_layer("layer_1").get_weights()
   model.get_layer("layer_1").set_weights([w, b]) # w, b为ndarray类型参数矩阵
   ~~~

4. 通过训练后的神经网络进行预测
   ~~~python
   X_test = np.array([
	   [200,13.9],  # postive example
       [200,17]])   # negative example
   X_testn = norm_l(X_test)  # Normalization
   predictions = model.predict(X_testn)
   ~~~
   **注意预测时一定要按照训练时的方式进行正则化**

> [!TIPS] `np.dot()`与`np.matmul()`区别
> 首先结论是优先用`np.matmul()`
> 
> `matmul()`不支持标量运算，即运算双方如果不是同一数据类型会报错，而`dot()`会转换为统一数据类型进行运算
> 
> PS: 也可以用$@$代替矩阵点乘

5. 手写一个神经网络层
   ~~~python
   # 循环写法
   def dense(a_in, W, b, g):
	   units = W.shape[1]
	   a_out = np.zeros(units)
	   for i in range(units):
		   a_out[i] = g(np.dot(W[:, i], a_in) + b[i])  # W[:,j]: 取出第j列
	   return a_out

   # 向量写法
   def dense(a_in, W, b, g):
	   return g(np.matmul(a_in, W) + b)
   ~~~
   为了便于理解，其实$W$这一参数矩阵样式为$\begin{bmatrix}-1&-3&5\\2&4&6\end{bmatrix}$，每一列代表着一组(一个单元)的参数$w_{1}, w_{2}$，因此$W.shape = (2, 3)$，$units = W.shape[1] = 3$


## 训练神经网络
> [!IMPORTANT] 重点两步
> `model.compile(loss=BinaryCrossentropy)`: 定义损失函数
> 
> `model.fit(X, Y, epochs=100)`: 定义训练集和训练次数

1. `BinaryCrossentropy()`(二元交叉熵损失)：应用于二分分类问题(binary classification)的最常用损失函数，实际上就是逻辑回归中的损失函数：$$L(f(\vec x), y) = -ylog(f(\vec x)) - (1 - y)log(1 - f(\vec x))$$$y$是训练集标签(label)，$f(\vec x)$是神经网络输出
2. `MeanSquaredError()`(均值平方误差)：应用于回归问题，计算平方损失

$$J(W, B) = \frac{1}{m}\sum\limits_{i = 1}^{m}L(f(\vec x), y^{(i)})$$被称为**交叉熵损失函数(cross entropy loss function)**，使用的训练方法即为**反向传播算法(back propagation)**

## 激活函数
> [!TIPS] 激活函数的意义
> 如果不使用激活函数而仅使用线性回归，线性函数的线性函数所得仍是一个线性函数，相当于我们没有对训练集做多层神经网络处理，而只是选取了某个单一参数。因此，在神经网络的隐藏层中尽量不要使用`linear activations`线性函数

> [!NOTE] 激活函数的选取
> 输出层对激活函数的选取一般取决于$Y$标签(label)的特征，如二分类$y$只有0或1，故`sigmoid`比较合适，正负均有的回归则线性回归比较合适，而仅有非负数的回归可以考虑`ReLU`
>
> 通常情况下建议隐藏层使用`ReLU`

1. `Sigmoid`函数：$$g(z) = \frac{1}{1+e^{-z}}$$
2. `ReLU`函数：代表整流线性单位(Rectified Linear Unit)，相对`sigmoid`的优势点在于仅有一段梯度消失区域，能更快进行梯度下降$$g(z) = max(0, z)$$
3. `LeakyReLU`函数：相较于`ReLU`对负轴部分加入了微小梯度(pytorch中$\alpha$默认0.01)，解决神经元死亡问题$$g(z)=\begin{cases}\quad x,\quad&if\quad x\geq0\\\alpha\times x,\quad&\text{otherwise}\end{cases}$$
4. `Linear activation`函数：(其实就是啥也没用)：$$g(z) = z$$
5. `Tanh`函数：利用双曲正切对数据进行激活，将元素调整到`(-1, 1)`区间$$g(z) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$$
6. `Swish`函数：导数恒大于0，防止慢速训练期间，梯度逐渐接近0并导致饱和；同时在优化泛化工作中，受益于其平滑度也能起到很好效果：$$g(z) = \frac{x}{1+e^{-z}}$$
7. `Softmax`函数：详见下一节多分类问题(Multiclass)：$$g(z)=\frac{e^{z}}{\sum\limits_{k=1}^{N}e^{z_{k}}}$$

## 多类分类问题(Multiclass Classification)
> [!SUMMARY] 定义
> 进行有限多类型的分类，即$Y$ label可以选取大于2个类别A

### Softmax回归算法

|  | Softmax回归 | 逻辑回归 |
| ---- | ---- | ---- |
| possibility | $a_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{N}e^{z_{k}}}=\mathrm{P(y=j\|\vec{x})}$ | $a_{1} = g(z) = \frac{1}{1+e^{-z}} = P(y=1\|\vec x)$<br>$a_{2} = 1 - a_{1} = P(y=0\|\vec x)$ |
| Loss function | $Loss = -log~a_{j},~~if~y = j$ | $Loss = -y~log~a_1-(1-y)log~a_2$ |
| Cost function | $Cost = J(\vec{w},b)=-\frac1m\left[\sum_{i=1}^m\sum_{j=1}^N1\left\{y^{(i)}==j\right\}\log\frac{e^{z_j^{(i)}}}{\sum_{k=1}^Ne^{z_k^{(i)}}}\right]$ | $Cost=J(\vec w, b)=\frac{1}{m}\sum\limits_{i=1}^{m}Loss\Big(f_{\vec{w},b}\Big(\vec{x}^{(i)}\Big),y^{(i)}\Big)$$ |

- 将`Softmax`激活函数应用于输出层以实现多分类任务，我们称神经网络有一个**软最大输出(Softmax output)**

- 在`tensorflow`中应用`Softmax`函数进行训练时，定义损失函数需要用到**稀疏范畴交叉熵函数(SparseCategoricalCrossentropy)**，`Categorical`表示进行多分类问题，`Sparse`则体现了每个类别仅能取分类中的一个类别label

> [!TIPS] 减少数字舍入误差(Numerical Roundoff Errors)
> 可以不显式计算中间量，直接将中间两结合到熵损失函数计算中
> 
> 如在逻辑回归中，可以考虑将输出层的激活函数设置为`linear`线性激活，然后在损失函数定义中添加参数`from_digits=True`
> 
> 注意：采用如上方式进行预测时，需要使用`predict_value = tf.nn.sigmoid(model(X))`这样的语法来进行预测

### 多标签分类问题(Multilabel Classification)
- 训练出一个神经网络具有多个输出，判断是否有多个分类标签存在


## 高级优化算法(Optimization)
---
> [!TIPS] 使用优化器的语法
> `model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=...)`

- `Adam`算法(Adaptive Moment estimation)：
   1. 当检测到梯度下降沿同一个相似方向下降，学习率过小时自动增大学习率$\alpha$；
   2. 在检测到梯度下降产生振荡(oscillating)时减小学习率$\alpha$使得梯度正常下降；
   3. 不使用全局学习率，对于每一个参数都有一个$\alpha$

## 其他神经网络层类型(Layer Types)
---
1. `Dense layer(密集层)`：前面所学的全部神经网络层均为密集层(全连接层)，即将上一层收到的激活值全部应用于下一层中进行计算
2. `Convolutional layer(卷积层)`：每个单元不对全部输入做操作，只对部分操作进行识别操作；具有更快的速度，需要更少的训练数据，不易过拟合

# 神经网络的误差分析(Diagnostic)
---
## 模型性能的评估
> [!NOTE] 划分训练集和测试集(以线性逻辑回归为例，即使用平方误差计算的模型)
> 最小化Cost函数：$J(\vec{w},b)={min}_{\vec{w},b}\left[\frac{1}{2m_{train}}\sum_{i=1}^{m_{train}}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^{2}+\frac{\lambda}{2m_{train}}\sum_{j=1}^{n}w_{j}^{2}\right]$
> 
> 评估测试集误差：$J_{test}(\vec w, b) = \frac{1}{2m_{test}}\biggl[\sum_{i=1}^{m_{test}}\bigl(f_{\vec{\mathrm{w}},b}\bigl(\vec{\mathrm{x}}_{test}^{(i)}\bigr)-y_{test}^{(i)}\bigr)^{2}\biggr]$
>
> **注意：对于训练和测试集的误差评估不包含正则化**

## 模型选择
> [!IMPORTANT] 交叉训练集
> 将数据集划分为训练集(training)、交叉验证集/开发集(cross validation)、测试集(test)
> 
> 其中，交叉验证集的误差被称为验证错误(validation error)$$J_{c\nu}(\vec{w},b)=\frac{1}{2m_{c\nu}}\left[\sum_{i=1}^{m_{c\nu}}\left(f_{\vec{w},b}\left(\vec{x}_{c\nu}^{(i)}\right)-y_{c\nu}^{(i)}\right)^{2}\right]$$

**通过查看最低的交叉验证集误差来选择模型**，而模型的泛化拟合误差(Estimate generalization error)则用测试集误差$J_{test}(\vec w, b)$来表示

## 偏差和方差(biad & variance)
1. 多项式次数$d$对偏差和方差的影响![](Pasted%20image%2020240128122825.png)
- 多项式次数$d$过小$\rightarrow$欠拟合(underfit)$\rightarrow$偏差过大(high bias)$\rightarrow$ $J_{train}$和$J_{cv}$都比较大
- 多项式次数$d$过大$\rightarrow$过拟合(overfit)$\rightarrow$方差过大(high variance)$\rightarrow$ $J_{train}$特别小，但$J_{cv}$较大

2. 正则化参数$\lambda$对偏差和方差的影响
   ![](Pasted%20image%2020240128144309.png)
   - 正则化参数$\lambda$过大(相当于所有参数都尽可能小，$f_{\vec x,b}\approx b$)$\rightarrow$欠拟合(underfit)$\rightarrow$偏差过大(high bias)$\rightarrow$ $J_{train}$和$J_{cv}$都比较大
   - 正则化参数$\lambda$过小(相当于没有进行正则化)$\rightarrow$过拟合(overfit)$\rightarrow$方差过大(high variance)$\rightarrow$ $J_{train}$特别小，但$J_{cv}$较大

### 建立表现基准(baseline level of performance)
- 通过一定的比较判断期望的误差能够最小达到多少，比较基准误差和训练集、交叉验证集的差距，来确定发生了偏差或是方差问题

### 学习曲线(learning curves)
1. 如果学习算法偏差较大，获得更多的训练集很可能并不会减小误差，因为模型并没有发生太大改变，依然不适用于这些数据![](Pasted%20image%2020240128150805.png)
2. 如果学习算法方差较大，获得更多的训练数据可能确实会有所帮助，因为更多的数据更加严谨的限定了曲线形状，而本身的曲线次数已经足以拟合出细微形状![](Pasted%20image%2020240128151127.png)

> [!IMPORTANT] 修正`high bias`和`high variance`的方法总结
> 修正高偏差问题：获取更多特征，提高多项式拟合次数，减小正则化参数$\lambda$
> 
> 修正高方差问题：获取更多训练集，减小训练特征维度，增大正则化参数$\lambda$

- 进行合适正则化后的大型神经网络相较于较小的神经网络并不会更多的出现过拟合问题，同时还能很好的解决欠拟合问题，即算力足够前提下更大的神经网络几乎没坏处。
- 在`tensorflow`中进行正则化的语法是在建立layers时加入参数 `kernel_regularizer=L2(lambda_value)`。其中$L1$正则化指参数$w$中各个元素绝对值之和，即产生稀疏权值矩阵用于特征选择；而$L2$正则化则与逻辑回归的正则化相同(平方和求平方根)，用于防止过拟合

## 添加数据的方式
1. 数据增强(data augmentation)：通过对已有的训练集做一些处理来产生新的训练集，如OCR图像识别中进行对比度改变，旋转等图像扭曲工作等失真变换，产生具有相同label的训练集。如将图像置于网格中引入网格的随机翘曲(random warpings)，来创建一个更为丰富的训练实例库；或是对音频添加一些环境噪声或是失真(distortions)类型等
2. 数据合成(data synthesis)：使用人工创建的全新的训练集来添加训练实例，例如使用计算机不同字体截图作为新的训练实例来增强对OCR任务的识别，主要用于图像识别任务

- 现代机器学习工作可以分为算法专注(Conventional model-centric approach)和数据专注(Data-centric approach)两种，在算法工作逐渐成熟的今天，获取数据这一工作受到越来越多的关注

## 迁移学习(transfer learning)
> [!NOTE] 原理
> 借助完成其他任务的模型，选取输出层之外的参数，更改输出层单元为匹配新任务的数量，进行训练。
> 
> 即先在较大训练集上训练神经网络(supervised pretraining)，再在较小训练集上进行参数调优(fine tuning)
> 1. 可以只训练输出层参数，适用于训练集较小的情况
> 2. 可以训练所有层参数，但将输出层之外的参数初始化为迁移模型的保留参数

> [!SUMMARY] 迁移学习的步骤
> 1. 下载在较大数据集上训练的相同类型预训练神经网络的参数
> 2. 在自己的个性化数据集上进行训练和参数微调

## 机器学习的项目流程

**确定项目范围(Scope project) $\rightarrow$ 收集数据(Collect data) $\rightarrow$ 训练模型(train model) $\rightarrow$ 发布项目(Deploy in production)![](Pasted%20image%2020240128184826.png)
- `MLOps`(machine learning operations)：构建(build)，部署(deploy)和维护(maintain)机器学习系统

## 对倾斜数据集(skewed datasets)的处理
倾斜数据集就可以理解为二分类问题中出现两种情况的概率并不均等，例如诊断一位患者是否患有某种罕见病，可能本身$y=1$的概率就仅有1%，但是我们最终得到的寻俩模型可能具有0.5%的误差，这时候就需要我们对倾斜数据集进行处理

在处理倾斜数据集问题时，我们通常使用不同的误差度量，而不仅仅是分类误差(classification error)来判断模型的好坏。

特别的，一组常见的误差度量标准是**精确度(precision)** 和 **召回率(recall)** 
1. 精确度等于 正确预测为某种输出的验证数 除以 预测为某种输出的总验证数$$Precision = \frac{\text{True positives}}{\# p r e d i c t e d~p o s i t i v e}=\frac{\text{True positives}}{\text{True pos + False pos}}$$
2. 召回率等于 正确预测为某种输出的验证数 除以 实际上为某种情况的总数$$Recall = \frac{\text{True positives}}{\# actual~p o s i t i v e}=\frac{\text{True positives}}{\text{True pos + False neg}}$$

**混淆矩阵(confusion matrix)** 是一个$2\times2$的矩阵，其中横轴表示实际类(二分类问题中即为1或0)；纵轴表示预测类，即模型在给定的例子中预测了什么![](Pasted%20image%2020240128192547.png)
### 准确率与召回率之间的权衡
1. 对于一些小概率事件分类，即我们希望有极高概率才预测为1时，可以提高决策边界阈值。因此提高了预测率，降低了召回率。这样我们可以尽可能减少被误认为的概率
2. 当我们希望尽可能对不确定的事件都预测为1时，可以降低决策边界阈值。因此降低了预测率，提高了召回率

> [!NOTE] F1分(F1 score)
> 更强调两者之间相对较小的一个，因为当两者中有一个值特别小的时候可能模型就不是特别有用。事实上，F1分其实就是$P$和$R$的调和平均值(harmonic mean)
> 
> $$F1 = \frac{1}{\frac{1}{2}(\frac{1}{P}+\frac{1}{R})} = \frac{2PR}{P+R}$$
> 

## 关于模型选择的lab记录

1. 关于数据导入
   对一维的`numpy`数组可以使用`np.expand_dims()`函数将类似`[0, 1, 2]`转化为矩阵形式的`[[0], [1], [2]]`以供后续运算
2. 关于划分训练集，验证集和测试集：
   `Scikit-learn`中有函数`train_test_split()`，具体调用：`x_train, x_test, y_train ,y_test = train_test_split(x, y, test_size=0.4, random_state=1)`
3. 关于特征缩放和计算偏差和标准差：
   1. `Scikit-learn`中调用`StandardScaler`类进行正则化同时计算$z$分数，其中$z = \frac{x-\mu}{\sigma}$(感觉理解成正态归一也行)，`fit_transform()`对数据集进行计算；调用计算类的`mean_`和`scale_`方法可以导出数据集的均值和标准差
   2. 值得注意的是，在进行验证集验证工作时，一定要用训练集进行正则化的参数来进行正则化！因此可以直接用之前的`scaler_linear`来进行操作，并且由于之前进行了`fit`操作，可以直接使用`transform(x_cv)`来进行正则化和计算相关参数
   ~~~python
   # Initialize the class
   scaler_linear = StandardScaler()
   
   # Compute the mean and standard deviation of the training set then transform it
   X_train_scaled = scaler_linear.fit_transform(x_train)
   
   print(f"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}")
   print(f"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}")
   ~~~
4. 关于模型评估
   1. 对于均值平方误差(MSE)可以直接使用`Scikit-learn`库中的函数：`J_train(w, b) = mean_squared_error(y_train, y_predict) / 2`
   2. 对于分类问题的误差，可以使用`np.mean(y_predict != y_actual)`函数来进行计算，相当于计算出错误预测数除以标签总数
5. 关于添加高阶多项式特征
   - 可以使用`PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)`类似的语法，这样会添加一个新的关于$x^{2}$的特征
   - 也可以在一定范围内测试添加多项式的度数，保存下每个模型的`cv_mse`，应用`np.argmin()`函数来选取最优模型

