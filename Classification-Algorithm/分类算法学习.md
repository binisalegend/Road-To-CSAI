# K-NN(K近邻)算法
---

> [!important] Title
> K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例**最邻近**的K个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。（**这就类似于现实生活中少数服从多数的思想**）
## 范围选取
- 首先选取临近范围时即不能太大也不能太小，范围太大导致太多数据进入选取范围，仅起到分类作用而不能辨别；范围太小导致误差过大，很容易出现过拟合

## 距离度量
- 度量多维空间中两点间距离：
   1. 设特征空间$\mathcal{X}$是$n$维实数向量空间$R^n$，$x_{i},x_{j}\in\mathcal{X}$，$x_{i}=\left(x_{i}^{(1)},x_{i}^{(2)},\cdots,x_{i}^{(n)}\right)^{\mathrm{T}}$，$x_{j}=\left(x_{j}^{(1)},x_{j}^{(2)},\cdots,x_{j}^{(n)}\right)^{\mathrm{T}}$，$x_i,x_j$的n维闵可夫斯基距离(Minkovski distance)距离$L_p$定义为：$$L_{p}(x_{i},x_{j})=\left(\sum_{l=1}^{n}\mid x_{i}^{(l)}-x_{j}^{(l)}\mid^{p}\right)^{\frac{1}{p}}$$
   2. 当上式中$p=2$时，称为欧氏距离(Euclidean distance)，即$$L_{2}(x_{i},x_{j})=\left(\sum_{l=1}^{n}\mid x_{i}^{(l)}-x_{j}^{(l)}\mid^{2}\right)^{\frac{1}{2}}$$
   3. 当$p=1$时，称为曼哈顿距离(Manhatten distance)，即$$L_{1}(x_{i},x_{j})=\sum_{l=1}^{n}|x_{i}^{(l)}-x_{j}^{(l)}|$$
   4. 当$p=\infty$时，即为各个坐标的最大值，即$$L_{_\infty}(x_{i},x_{j})=\max_{l}\mid x_{i}^{(l)}-x_{j}^{(l)}\mid$$

## 特征归一化
- 一般来说，假设进行kNN分类使用的样本特征是$\{(x_{i1},x_{i2},\ldots,x_{in})\}_{i=1}^m$，取每一轴上的最大值减去最小值$$M_j=\max_{i=1,\ldots,m}x_{ij}-\min_{i=1,\ldots,m}x_{ij}$$并且在计算距离时将每一个坐标轴除以相应的$M_j$进行归一化，即$$d((y_1,\ldots,y_n),(z_1,\ldots,z_n))=\sqrt{\sum_{j=1}^n\left(\frac{y_j}{M_j}-\frac{z_j}{M_j}\right)^2}$$

# K-means(K均值)算法
---

> [!TITLE] K-means聚类
> 目标：将$N$个观测值划分到$K$个集合中
> 
> 步骤：
> 1. 随机选择$K$个观测值作为初始的聚类中心
> 2. 对于每一个观测值都计算它到$K$个聚类中心的距离，并将其划分到距离最近的聚类中心所在的集合
> 3. 对于每个聚类中心的集合，计算集合中所有观测值的位置均值，更新聚类中心
> 4. 重复步骤2、3，直到聚类中心几乎不再变化或者达到预定迭代次数

