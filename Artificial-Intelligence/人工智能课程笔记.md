# 机器学习(Machine Learning)
---
> Improve on *T*ask with respect to *P*erformance metric based on *E*xperience

- 主要从效果(Effectiveness)和效率(efficiency of the computation)两方面衡量

机器学习的必要性：未知环境和复杂系统，大规模数据挖掘，促进人类智能

**Objectiveness**和**Optimization**

## 监督学习
> [!SUMMARY] 基本思想
> 给定$x$和$f(x)$，试图求解出一个假设的(*hypothesis*)映射函数$x$逼近$f$，其主要任务如下：
> 1. 如何定义函数形式
> 2. 如何优化函数参数和函数结构

- Mean Square Error(MSE)：均值平方根误差

Ockham's Razor(奥卡姆剃刀原理)：在逼近数据的前提下令函数尽可能简单，使函数具有普适性

### 决策树(Decision Tree)
选择作为根节点的特征：以某个特征能将子节点分成几个独立性更强的部分

- Entropy(熵)：从函数**结果**进行计算$$I(\frac p{p+n},\frac n{p+n})=-\frac p{p+n}\log_2\frac p{p+n}-\frac n{p+n}\log_2\frac n{p+n}$$熵越大表示区分数据越混乱，越小表示越独立
- Remainder Entropy(剩余熵)：$$R(A) = \sum_{i=1}^v\frac{p_i+n_i}{p+n}I(\frac{p_i}{p_i+n_i},\frac{n_i}{p_i+n_i})$$
- 信息增益(Information Gain)：选择`IG`最大的特征作为根节点特征 $$IG(A) = I\left(\frac p{p+n},\frac n{p+n}\right)- R(A)$$

#### 预防过拟合(Overfitting)
1. 扩大数据集
2. 提前停止拟合
3. 允许在数据集上过拟合，但随后对模型做剪枝
   - 通过做DL(Description Length)分析，选取Code Length更低的决策树

### 贝叶斯推论
> [!NOTE] 贝叶斯定理
> $$P\left(h|D\right)=\frac{P\left(D,h\right)}{P\left(D\right)}=\frac{P\left(D|h\right)P\left(h\right)}{P\left(D\right)}$$其中$D$表示数据，$h$表示假设；$P(h|D)$表示后验概率(Posterior Probability)，$P(h)$表示先验概率(Prior Probability)，$P(D|h)$表示类条件概率(Class-conditional Probability)

- MAP(Maximum A Posterior)：最大后验估计，通过计算每个候选假设的后验概率找到最大项

## 无监督学习



# 人工神经网络(Artificial Neural Network)
---



# 搜索与问题求解(Search and Problem Solving)
---




# 知识与推理(Knowledge and Reasoning)
---



# 进化计算(Evolutionary Computation)
---



# 群体智能(Swarm Intelligence)
---




# 新型人工智能(Nouvelle AI)
---
