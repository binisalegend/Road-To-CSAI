# 强化学习概念介绍
简单理解来说是一种通过试错来确定完成某项任务的最佳步骤的方法，其学习系统没有办法像一些机器学习一样被告知应该做出什么样的行为，需要通过不断尝试来发现如何实现奖赏值的最大化；当前行为不仅影响即时奖励，同样会影响下一步包括最终可能获得的奖励
Concept：找到一个最佳策略 `policy` ,可以让本体 `agent` 在特定环境 `environment` 中，根据当前状态 `state`,做出行为 `action`, 从而获得最大回报 `Return`
# 机器狗强化学习实例([RoomDog.py]("D:\大学资料\Road-To-CSAI\ReinforcementLearning\RoomDog.py"))
学习链接：[一个简单的强化学习例子来理解Q-learning](https://zhuanlan.zhihu.com/p/36669905)  [强化学习Q-Learning算法学习笔记](https://blog.csdn.net/qq_43655453/article/details/107296374)
- 应用算法: Q-Learning算法
  术语：`state`(状态)，`action`(行为)
- 丢！终于看懂怎么转化矩阵了，如下：
  1. 首先，简单思路是有几个房间，矩阵就有几行几列
  2. 可以将行列分别理解成 `state` 和 `action`，即例如从房间4去房间2就可以用矩阵元素[4, 2]的状态来表示
  3. 在路径连通与否的表示上，如果从A能到B，则[A,B]就表示为0，反之为-1；特别的，如果B是最终要到达的目的地，此时AB连通就应表示为一个较大的奖赏值R
像题目中的房间联通情况![[Pasted image 20231008121208.png]]可以表示为
$$
\mathbb{R}=\begin{bmatrix}-1&-1&-1&-1&0&-1\\-1&-1&-1&0&-1&100\\-1&-1&-1&0&-1&-1\\-1&0&0&-1&0&-1\\-1&0&-1&0&-1&100\\0&-1&-1&-1&0&100\end{bmatrix}
$$
- 贝尔曼方程：$$Q(s,a)=R(s,a)+\gamma\bullet\max\{Q(\widetilde{s},\widetilde{a})\}$$
其中$Q(s, a)$表示当前的状态和行为，$Q(\widetilde{s},\widetilde{a})$表示下一过程的状态和行为,$\gamma$表示位于0 1之间的折扣系数，表示模型的远见程度，$\gamma$越小表示当下的Reward奖励函数越比未来的重要
- <big>Q-Learning的算法步骤</big>
 1. 设置γ参数和R矩阵的环境奖励
 2. 初始化矩阵Q为0
 3. 对于每一个状态
    (1) 随机选择一个状态
    (2) do (while unreached the goal)
    a. 在当前状态所有可能的行动中选择一个
    b. 使用这个可能行动然后分析到达下一个状态
    c. 基于所有在当前状态下可能的行动获得最大值Q
    d. 理由贝尔曼方程计算当前状态下的Q
	